{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import pandas as pd\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv',sep=';')\n",
    "df_test = pd.read_csv('train.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sent):\n",
    "    stemmed = []\n",
    "    for word in sent:\n",
    "        stemmed.append(stemmer.stem(word))\n",
    "    sent = nltk.pos_tag(stemmed)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = preprocess(df_test[\"Word\"].values)\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN\n",
    "  PP: {<IN><NP>}               # Chunk prepositions followed by NP\n",
    "  VP: {<VB.*><NP|PP|CLAUSE>+$} # Chunk verbs and their arguments\n",
    "  CLAUSE: {<NP><VP>}           # Chunk NP, VP\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser(grammar)\n",
    "cs = cp.parse(sent)\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent = preprocess(df_train[\"Word\"].values)\n",
    "stemmed,postags  = zip(*test_sent)\n",
    "df_train[\"Word\"] = stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train = df_train[df_train.Tag != \"O\"]\n",
    "\n",
    "df_train = df_train[(df_train.POS == \"NN\") | (df_train.POS == \"NNP\") | (df_train.POS == \"NNS\") | (df_train.POS == \"NNPS\") |\n",
    "        (df_train.POS == \"CD\") |\n",
    "        (df_train.POS == \"JJ\") | (df_train.POS == \"JJR\") | (df_train.POS == \"JJS\")]\n",
    "\n",
    "df_train[\"Tag\"] = df_train[\"Tag\"].replace(\n",
    "    {\"B-per\":\"per\",\n",
    "     \"I-per\":\"per\",\n",
    "          \"B-event\":\"event\",\n",
    "          \"I-event\":\"event\",\n",
    "          \"B-geo\":\"geo\",\n",
    "          \"I-geo\":\"geo\",\n",
    "          \"B-gpe\":\"gpe\",\n",
    "          \"I-gpe\":\"gpe\",\n",
    "          \"B-obj\":\"obj\",\n",
    "          \"I-obj\":\"obj\",\n",
    "          \"B-org\":\"org\",\n",
    "          \"I-org\":\"org\",\n",
    "          \"B-time\":\"time\",\n",
    "          \"I-time\":\"time\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_occurence = df_train.groupby([\"Word\"]).count()\n",
    "sum_occurence.loc[\"nato\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, value in sum_occurence.items():\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_probab = df_train.groupby(\"Word\").Tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, value in df_probab.items():\n",
    "    df_probab.loc[index] = value / sum_occurence.loc[index[0]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_probab.loc[\"nato\"]\n",
    "df_probab.loc[\"nato\"]\n",
    "for wat in df_probab.loc[\"nato\"].index:\n",
    "    print(wat, df_probab.loc[\"nato\"][wat])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in cs.subtrees(lambda t: t.label() == \"NP\"):\n",
    "    probab = {\"per\": 0, \"event\": 0, \"geo\":0, \"org\":0, \"obj\":0, \"gpe\":0, \"time\":0}\n",
    "    num_of_leaves = len(s.leaves())\n",
    "    for word, pos in s.leaves():\n",
    "        if word in df_probab.index:\n",
    "            for tag_type in df_probab.loc[word].index:\n",
    "                probab[tag_type] += df_probab.loc[word][tag_type] / num_of_leaves\n",
    "    if max(probab.items(), key=operator.itemgetter(1))[1] >= 0.5:\n",
    "        print(max(probab.items(), key=operator.itemgetter(1))[0], s.leaves())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
